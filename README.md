### 1
- –§—É–Ω–∫—Ü–∏—è compute_derivative –Ω–∞ –≤—Ö–æ–¥ –ø—Ä–∏–º–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é `y=f(x)` –≤–∏–¥–∞ `x**2 - x + 21`  
- –í–µ—Ä–Ω—ë—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é: `2ùë•‚àí1`

### 2
–§—É–Ω–∫—Ü–∏—è compute_partial_derivative() –Ω–∞ –≤—Ö–æ–¥ –ø—Ä–∏–º–µ—Ç:
- –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é `p = f(x, y)`–∑–∞–≤–∏—Å—è—â—É—é –æ—Ç –¥–≤—É—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤–∏–¥–∞ `(x-y)**2`;
- –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é.
–§—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.

### 3
–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º —É –Ω–∞—Å –µ—Å—Ç—å:
- –∏—Å—Ç–∏–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–π `y_true`  
- –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ `y_pred`, –æ–ø–∏—Å–∞–Ω–Ω–æ–µ –ª–∏–Ω–µ–π–Ω–Ω—ã–º —É—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –≤–∏–¥–∞ `y_pred = w*x + b`
1. –í—ã–≤–µ–¥–∏—Ç–µ —Ñ–æ—Ä–º—É–ª—É –∫–≤–∞–¥—Ä–∞—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –º–µ–∂–¥—É –∏—Å—Ç–∏–Ω–Ω—ã–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º–∏. 
2. –í—ã—á–∏—Å–ª–∏—Ç–µ —á–∞—Å—Ç–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω–Ω–æ–π —Ñ–æ—Ä–º—É–ª—É –∫–≤–∞–¥—Ä–∞—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –ø–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º `w` –∏ `b`.

### 4
1. –í —ç—Ç–æ—Ç —Ä–∞–∑ —Ç–µ–±–µ –Ω—É–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å –∫–ª–∞—Å—Å `Gradient()`, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç —Å–æ—Å—Ç–æ—è—Ç—å –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤. –û–±—ä–µ–∫—Ç –ø—Ä–∏ 
–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–ª—É—á–∞–µ—Ç –¥–≤–∞ –≤–µ–∫—Ç–æ—Ä–∞: `X`, `Y`. –î–∞–ª–µ–µ –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞—â–∞—é—Ç—Å—è –∫ –Ω–∏–º —É–∂–µ –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Å–∞: 
     * `predict`: –Ω–∞ –≤—Ö–æ–¥ –æ–Ω–∞ –ø–æ–ª—É—á–∞–µ—Ç `w`, `b`, –Ω–∞ –≤—ã—Ö–æ–¥ –æ–Ω–∞ –≤—ã–¥–∞—ë—Ç –ø—Ä–æ–≥–Ω–æ–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è `Y_pred`;
     * `mse`: –Ω–∞ –≤—Ö–æ–¥ –æ–Ω–∞ –ø–æ–ª—É—á–∞–µ—Ç `Y_pred`, –Ω–∞ –≤—ã—Ö–æ–¥ –≤—ã–¥–∞–µ—Ç –ø–æ—Å—á–∏—Ç–∞–Ω–Ω–æ–µ MSE;
     * `update`: –Ω–∞ –≤—Ö–æ–¥ –æ–Ω–∞ –ø–æ–ª—É—á–∞–µ—Ç `w`, `b` –∏ `a` (–Ω–∞—à learning rate). –°–¥–µ–ª–∞–π—Ç–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∑–Ω–∞—á–µ–Ω–∏–µ `a=0.0001`. –ê –Ω–∞ \ 
–≤—ã—Ö–æ–¥e –º–µ—Ç–æ–¥ –≤—ã–¥–∞—ë—Ç –Ω–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è `w`, `b`, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–Ω–æ–≤–∏–ª–∏—Å—å –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º.
2. –ü—Ä–æ–≤–µ—Ä—å —Ä–∞–±–æ—Ç—É –º–µ—Ç–æ–¥–æ–≤, –ø–æ–¥–∞–≤ –Ω–∞ –≤—Ö–æ–¥ –∫–ª–∞—Å—Å–∞ –¥–≤–∞ –≤–µ–∫—Ç–æ—Ä–∞ `X`, `Y` –∏ –Ω–∞—á–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã `w`, `b` –≤ —Ç—Ä–µ–±—É–µ–º—ã–µ –º–µ—Ç–æ–¥—ã.

### 5
–ú–æ–¥–µ—Ä–Ω–∏–∑–∏—Ä—É–µ–º –∫–æ–¥  —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –ø–æ—è–≤–∏–ª—Å—è –º–µ—Ç–æ–¥ `optimize`. –ù–∞ –≤—Ö–æ–¥ –º–µ—Ç–æ–¥ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç `num_iterations`, 
`stopping_threshold=80`, `a=0.000001`. 
–ú–µ—Ç–æ–¥ –¥–æ–ª–∂–µ–Ω –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –ø—Ä–æ–π—Ç–∏ –∫–∞–∫–æ–µ-—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π `w`, `b`, 
–ø—Ä–∏–¥—è –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é. –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏:
  * –µ—Å–ª–∏ –±—ã–ª–æ –ø—Ä–µ–≤—ã—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞—Ä–∞–Ω–µ–µ –∑–∞–¥–∞–Ω–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π `num_iterations`;
  * –µ—Å–ª–∏ –Ω–æ–≤–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è –≤—ã–¥–∞–ª–∞ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É —Ç–µ–∫—É—â–µ–π –æ—à–∏–±–∫–æ–π –∏ –æ—à–∏–±–∫–æ–π –ø—Ä–æ—à–ª–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∑–Ω–∞—á–µ–Ω–∏–µ, –º–µ–Ω—å—à–µ–µ, —á–µ–º `stopping_threshold`;
  * –º–µ—Ç–æ–¥ –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è `w`, `b`, –∏ `mse`.

_________________________________________________________________________________________________________________________________________________

### 1
- The input compute_derivative function will take the mathematical function 'y = f (x)' of the form 'x * * 2 - x + 21'
- Returns derivative: '2ùë•‚àí1'

### 2
The compute_partial_derivative () function will accept:
- mathematical function 'p = f (x, y)' dependent on two variables of the form '(x-y) * * 2';
- is the variable by which you want to calculate the partial derivative.
The function returns a derivative of the selected variable.

### 3
Suppose we have:
- is the true value of'y _ true '
- is the predicted value 'y _ pred' described by a linine equation of the form 'y _ pred = w * x + b'
1. Derive the square deviation formula between the true and predicted values.
2. Calculate the partial derivatives of the resulting square deviation formula over the variables' w'and' b '.

### 4
1. This time you need to write the class 'Gradient ()', which will consist of several methods. Object at
initialization gets two vectors: 'X', 'Y'. Further, methods refer to them already inside the class:
* 'predict': at the input it receives 'w','b', at the output it produces predictive values ‚Äãof ' Y _ pred ';
* 'mse': it receives 'Y _ pred' at the input, gives the calculated MSE to the output;
* 'update': It receives 'w', 'b', and' a'as input (our learning rate). Make the default value'A = 0.0001 '. A on
the output of the method produces new values ‚Äã ‚Äã of 'w','b', which have been updated due to the calculated gradients.
2. Check the operation of the methods by supplying two vectors 'X', 'Y' and the initial parameters 'w', 'b' to the required methods to the class input.

### 5
We upgrade the code so that the'optimize' method appears. At the input, the method takes 'num_iterations',
`stopping_threshold=80`, `a=0.000001`.
The method must iteratively go through some number of times to update the values ‚Äã ‚Äã of 'w', 'b',
having reached the optimal value. Stopping criteria:
* if the number of predefined iterations 'num _ iterations' has been exceeded;
* if the new iteration returned a difference between the current error and the previous iteration error of less than 'stopping _ threshold';
* method must return the final values 'w','b', and 'mse'.